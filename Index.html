<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Advanced Classroom AI Tracker (Fixed)</title>

    <!-- 1. Tailwind CSS for modern, responsive styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    
    <!-- 2. Custom CSS from style.css -->
    <style>
        /* Variables from style.css */
        body {
            --primary-color: #1e3a8a; /* Deep Blue */
            --secondary-color: #fcd34d; /* Yellow */
            --success-color: #059669; /* Green */
            --danger-color: #dc2626; /* Red */
            --warning-color: #ca8a04; /* Darker Yellow */
        }
        
        body {
            background-color: #f6f7fb;
            color: #1e293b;
            font-family: 'Inter', sans-serif;
        }

        /* Ensure the canvas fills the container and is centered */
        #canvas {
            width: 100%;
            height: 100%;
            object-fit: contain; 
            border-radius: 6px;
            background-color: #000; /* Black background for video feed */
        }

        input[type=range] {
            accent-color: var(--primary-color);
        }

        .school-header {
            background-color: var(--primary-color);
            border-bottom: 3px solid #172554;
        }

        .hud-panel {
            background: #ffffff;
            border: 1px solid #d7dce3;
            box-shadow: 0 2px 6px rgba(0,0,0,0.05);
        }

        .school-footer {
            background-color: #e2e8f0;
            color: #475569;
        }

        #loader {
            background-color: rgba(255, 255, 255, 0.95);
        }
    </style>

    <!-- 3. Machine Learning Libraries (Needed in Main Thread for some utils/initialization) -->
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script> 
    <!-- Face-API is now only needed inside the Worker via importScripts -->
</head>

<body class="h-screen flex flex-col overflow-hidden">

    <!-- Header and HUD (Fully Responsive) -->
    <div class="school-header p-4 flex flex-wrap justify-between items-center gap-4">
        <div>
            <h1 class="text-xl font-bold text-white tracking-widest">ADVANCED<span class="text-yellow-300">AI</span></h1>
            <p class="text-xs text-blue-200">Attention & Motion Tracking</p>
        </div>

        <!-- HUD Panel -->
        <div class="flex items-center gap-4 bg-white px-4 py-2 rounded-lg shadow hud-panel">
            <label class="text-xs font-semibold whitespace-nowrap">Focus Sensitivity</label>
            <input type="range" id="sensitivity" min="10" max="90" value="50" class="w-20 sm:w-32">
            <span id="senseVal" class="text-sm font-bold w-10 text-right">50%</span>
        </div>

        <!-- Metric Display -->
        <div class="flex gap-4 sm:gap-6 text-white">
            <div class="text-center">
                <p id="countDisplay" class="text-2xl font-bold leading-none">0</p>
                <p class="text-xs text-blue-200">Total Students</p>
            </div>
            <div class="text-center">
                <p id="focusDisplay" class="text-2xl font-bold leading-none">0%</p>
                <p class="text-xs text-blue-200">Focused Score</p>
            </div>
        </div>
    </div>

    <!-- Main Content Area (Video/Canvas) -->
    <div class="flex-grow relative flex items-center justify-center overflow-hidden">
        <!-- The video element is hidden but feeds frames to the canvas/worker -->
        <video id="video" class="absolute hidden" playsinline autoplay></video>
        <!-- The canvas displays the video feed AND the detection overlays -->
        <canvas id="canvas"></canvas>

        <!-- Start/Stop Button Overlay -->
        <div class="absolute z-40 p-4">
            <button id="initBtn" onclick="initSystem()" disabled
                class="px-6 py-3 bg-yellow-500 text-slate-900 font-bold rounded-xl shadow-lg hover:bg-yellow-400 transition disabled:opacity-50 disabled:cursor-not-allowed">
                Initializing System...
            </button>
        </div>

        <!-- Loading Spinner -->
        <div id="loader" class="absolute inset-0 flex flex-col items-center justify-center z-30">
            <div class="w-16 h-16 border-4 border-blue-500 border-t-transparent rounded-full animate-spin mb-4"></div>
            <h2 class="text-xl font-bold">LOADING SYSTEM</h2>
            <p class="text-sm mt-2 text-gray-600">Initializing Camera & Backendâ€¦</p>
        </div>
    </div>

    <!-- Footer Status Bar -->
    <div class="school-footer p-2 text-[10px] flex flex-wrap justify-center gap-4 sm:gap-6 border-t border-slate-300">
        <span class="flex items-center gap-1 font-semibold">
            <div class="w-2 h-2 bg-green-500 rounded-full"></div> Focused
        </span>
        <span class="flex items-center gap-1 font-semibold">
            <div class="w-2 h-2 bg-yellow-500 rounded-full"></div> Writing / Low Attention
        </span>
        <span class="flex items-center gap-1 font-semibold">
            <div class="w-2 h-2 bg-red-500 rounded-full"></div> Distracted / Sleeping
        </span>
        <span class="flex items-center gap-1 font-semibold">
            <div class="w-2 h-2 bg-slate-500 rounded-full"></div> Unidentified
        </span>
    </div>

    <!-- 4. Combined JavaScript (Main Thread + Dynamic Web Worker) -->
    <script>
        // --- Shared Constants & UI Elements ---
        const video = document.getElementById("video");
        const canvas = document.getElementById("canvas");
        const ctx = canvas.getContext("2d");
        const loader = document.getElementById("loader");
        const countDisplay = document.getElementById("countDisplay");
        const focusDisplay = document.getElementById("focusDisplay");
        const sensitivitySlider = document.getElementById("sensitivity");
        const senseVal = document.getElementById("senseVal");
        const initBtn = document.getElementById("initBtn");
        
        // Tracking state
        let trackers = []; // Array of StudentTracker instances
        let nextID = 0; // Unique ID for new trackers

        // State flags
        let systemInitialized = false;
        let modelsLoaded = false;
        let loopActive = false;
        let worker;

        // UI setup for sensitivity slider
        senseVal.innerText = sensitivitySlider.value + '%';
        sensitivitySlider.oninput = (e) => {
            senseVal.innerText = e.target.value + '%';
        };

        // --- PART 1: TRACKING.JS (Kalman Filter and Tracker Logic) ---

        /**
         * Kalman Filter implementation for predicting and correcting position.
         * Uses TensorFlow.js for linear algebra operations.
         */
        class Kalman {
            constructor() {
                // State vector [x, y, dx, dy] - Position and Velocity (as a Tensor)
                this.x = tf.tensor([[0],[0],[0],[0]]);
                // Covariance matrix P (High initial uncertainty)
                this.P = tf.eye(4).mul(100); 

                // State transition matrix F (constant velocity model, dt=1 frame)
                this.F = tf.tensor([[1,0,1,0],
                                    [0,1,0,1],
                                    [0,0,1,0],
                                    [0,0,0,1]]);

                // Observation matrix H (we only observe x and y positions)
                this.H = tf.tensor([[1,0,0,0],
                                    [0,1,0,0]]);

                // Measurement noise R (Tuned for typical webcam noise)
                this.R = tf.eye(2).mul(0.1);
                // Process noise Q (Tuned for slight acceleration/deceleration uncertainty)
                this.Q = tf.eye(4).mul(0.01);
            }

            /** Predicts the next state (time update). */
            predict() {
                tf.tidy(() => {
                    // Predict next state: x_k = F * x_{k-1}
                    this.x = this.F.matMul(this.x);
                    // Predict next covariance: P_k = F * P_{k-1} * F^T + Q
                    this.P = this.F.matMul(this.P).matMul(this.F.transpose()).add(this.Q);
                });
            }

            /** Corrects the state based on a new measurement (measurement update). 
             * @param {number} x - Measured X position.
             * @param {number} y - Measured Y position.
             */
            correct(x, y) {
                tf.tidy(() => {
                    // Measurement vector Z
                    const Z = tf.tensor([[x], [y]]);

                    // Innovation: y_tilde = Z - H * x_k
                    const y_tilde = Z.sub(this.H.matMul(this.x));

                    // Innovation covariance: S = H * P_k * H^T + R
                    const S = this.H.matMul(this.P).matMul(this.H.transpose()).add(this.R);

                    // Kalman Gain: K = P_k * H^T * S^{-1}
                    const K = this.P.matMul(this.H.transpose()).matMul(tf.linalg.solve(S, tf.eye(2)));

                    // Updated state: x_{k} = x_k + K * y_tilde
                    this.x = this.x.add(K.matMul(y_tilde));

                    // Updated covariance: P_{k} = (I - K * H) * P_k
                    const I = tf.eye(4);
                    const I_minus_KH = I.sub(K.matMul(this.H));
                    this.P = I_minus_KH.matMul(this.P);
                });
            }

            /** @returns {object} The predicted position {x, y}. */
            get position() {
                // Returns the first two elements [x, y] of the state vector
                return {
                    x: this.x.dataSync()[0],
                    y: this.x.dataSync()[1]
                };
            }
        }


        /**
         * Tracks a single student's position, state, and identity.
         */
        class StudentTracker {
            constructor(id, x, y) { // Removed 'name' since identity is removed
                this.id = id;
                this.kalman = new Kalman();
                this.kalman.correct(x, y); // Initialize state with first position
                this.box = null; // Bounding box
                this.state = 'Unknown';
                this.name = `Student ${id}`; // Default name since recognition is disabled
                this.lost = 0;
            }

            /**
             * Updates the tracker with a new detection.
             * @param {number} x - Detected X position.
             * @param {number} y - Detected Y position.
             * @param {string} state - Attention state (Focused, Writing, etc.).
             * @param {object} box - The bounding box object.
             */
            update(x, y, state, box) { // Removed 'name'
                this.kalman.correct(x, y);
                this.state = state;
                this.box = box;
                this.lost = 0;
            }

            /**
             * Checks if the student is considered focused based on state.
             * @returns {boolean}
             */
            isFocused() {
                return this.state === 'Focused' || this.state === 'Writing';
            }
        }

        /**
         * Matches new detections to existing trackers and updates the Kalman state.
         * @param {Array} dets - Array of new detections from the worker.
         */
        function matchStudents(dets) {
            // 1. Prediction: Use the Kalman filter to predict the next position for all existing trackers
            trackers.forEach(t => t.kalman.predict()); 
            const assigned = new Set();

            for (let d of dets) {
                let bestTracker = null;
                let bestDist = 200; // Max distance for association in pixels

                for (let t of trackers) {
                    // Compare new detection location (d.x, d.y) to the tracker's PREDICTED position
                    const p = t.kalman.position;
                    const dist = Math.hypot(d.x - p.x, d.y - p.y);

                    // Match if close enough and the tracker hasn't been assigned yet
                    if (dist < bestDist && !assigned.has(t.id)) {
                        bestDist = dist;
                        bestTracker = t;
                    }
                }

                if (bestTracker) {
                    // Found a match: Update the existing tracker
                    bestTracker.update(d.x, d.y, d.state, d.box);
                    assigned.add(bestTracker.id);
                } else {
                    // No match: Create a new tracker
                    trackers.push(
                        new StudentTracker(nextID++, d.x, d.y)
                    );
                }
            }

            // 2. Manage lost trackers
            for (let i = trackers.length - 1; i >= 0; i--) {
                if (!assigned.has(trackers[i].id)) {
                    trackers[i].lost++;
                    // Remove if lost for more than 30 frames (approx 1 second)
                    if (trackers[i].lost > 30) {
                        // IMPORTANT: Dispose of Tensors managed by the tracker's Kalman filter
                        tf.dispose([trackers[i].kalman.x, trackers[i].kalman.P]); 
                        trackers.splice(i, 1);
                    }
                }
            }
        }


        // --- PART 2: MAIN.JS (UI, Video Setup, Worker Communication) ---

        /** Initializes the camera stream and system. */
        async function initSystem() {
            if (systemInitialized) {
                // Toggle streaming (if already initialized)
                loopActive = !loopActive;
                initBtn.textContent = loopActive ? 'STOP STREAM' : 'START STREAM';
                if (loopActive) {
                    requestAnimationFrame(renderLoop);
                }
                return;
            }
            
            initBtn.disabled = true;
            initBtn.textContent = 'Starting Camera...';

            try {
                // Request video stream from camera
                const stream = await navigator.mediaDevices.getUserMedia({
                    video: { 
                        facingMode: "user", // Front camera
                        width: 640, 
                        height: 480 
                    }, 
                    audio: false 
                });
                video.srcObject = stream;

                // Wait for video metadata to load to get intrinsic size
                await new Promise(resolve => video.onloadedmetadata = resolve);

                // Set canvas size to match video feed size
                canvas.width = video.videoWidth;
                canvas.height = video.videoHeight;
                video.play();
                
                // Finalize setup
                systemInitialized = true;
                loopActive = true;
                initBtn.textContent = 'STOP STREAM';
                loader.classList.add('hidden');
                
                // Start the main rendering/processing loop
                requestAnimationFrame(renderLoop);

            } catch (err) {
                console.error("Failed to access camera: ", err);
                loader.innerHTML = `<h2 class="text-xl font-bold text-red-600">CAMERA ERROR</h2><p class="text-sm mt-2">Could not access webcam. Please check permissions.</p>`;
            }
        }

        /** The main application loop. */
        function renderLoop() {
            if (!loopActive) return;

            // 1. Draw the video frame to the canvas (for display)
            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
            
            // 2. Send the image to the Web Worker for processing
            // createImageBitmap efficiently transfers the image data without copying
            createImageBitmap(video).then(imageBitmap => {
                worker.postMessage({
                    type: "process",
                    image: imageBitmap
                }, [imageBitmap]); // Transferable object
            }).catch(e => console.error("Error creating ImageBitmap:", e));

            // 3. Draw the tracking overlays on the canvas
            drawTrackers();

            // 4. Update the HUD
            updateHUD();

            // 5. Loop again
            requestAnimationFrame(renderLoop);
        }

        /** Draws bounding boxes and state labels for all tracked students. */
        function drawTrackers() {
            ctx.lineWidth = 3;
            ctx.font = '20px Inter, sans-serif';
            ctx.textBaseline = 'top';

            trackers.forEach(t => {
                let color;
                let stateText;
                let labelText;

                // Determine color and state label
                switch (t.state) {
                    case 'Focused':
                        color = 'var(--success-color)';
                        stateText = 'Focused';
                        break;
                    case 'Writing':
                        color = 'var(--warning-color)';
                        stateText = 'Low Attention';
                        break;
                    case 'Looking Away':
                    case 'Sleeping':
                        color = 'var(--danger-color)';
                        stateText = 'Distracted';
                        break;
                    default:
                        color = 'gray';
                        stateText = 'Unidentified';
                }
                
                labelText = `${t.name} (${stateText})`;

                // 1. Draw Bounding Box (Received from worker)
                const box = t.box;
                if (box) {
                    // Scale box coordinates to match the canvas size (if different from 640x480)
                    const x = box.xMin * canvas.width / 640;
                    const y = box.yMin * canvas.height / 480;
                    const w = box.width * canvas.width / 640;
                    const h = box.height * canvas.height / 480;

                    ctx.strokeStyle = color;
                    ctx.strokeRect(x, y, w, h);
                

                    // 2. Prepare Label Background (Name + State)
                    ctx.fillStyle = color;
                    const textWidth = ctx.measureText(labelText).width;
                    // Position the label above the bounding box
                    ctx.fillRect(x, y - 30, textWidth + 10, 30);

                    // 3. Draw Label Text
                    ctx.fillStyle = 'white';
                    ctx.fillText(labelText, x + 5, y - 25);
                }
                
                // 4. Draw Smoothed Position (Kalman Prediction)
                const p = t.kalman.position;
                ctx.fillStyle = color;
                ctx.beginPath();
                ctx.arc(p.x * canvas.width / 640, p.y * canvas.height / 480, 5, 0, Math.PI * 2);
                ctx.fill();
            });
        }

        /** Updates the dashboard metrics. */
        function updateHUD() {
            const totalStudents = trackers.length;
            let focusedCount = 0;
            
            trackers.forEach(t => {
                if (t.isFocused()) {
                    focusedCount++;
                }
            });

            const focusedPct = totalStudents > 0 ? Math.round((focusedCount / totalStudents) * 100) : 0;
            
            // Update displays
            countDisplay.innerText = totalStudents.toString();
            focusDisplay.innerText = focusedPct + '%';
            
            // Update color based on focus level and sensitivity
            const strictness = parseInt(sensitivitySlider.value, 10);
            const focusThreshold = 100 - strictness; 

            focusDisplay.className = 'text-2xl font-bold leading-none ';
            
            if (focusedPct >= focusThreshold) {
                focusDisplay.classList.add('text-green-600');
            } else if (focusedPct >= focusThreshold - 20) {
                focusDisplay.classList.add('text-yellow-600');
            } else {
                focusDisplay.classList.add('text-red-600');
            }
        }
        
        // --- PART 3: DYNAMIC WEB WORKER SETUP (Combining all worker-side scripts) ---

        // This string contains the content of gaze.js, classifier.js (MODIFIED), and worker.js (MODIFIED)
        const workerCode = `
            // Load necessary external scripts (TFJS, Face-Landmarks-Detection)
            importScripts("https://cdn.jsdelivr.net/npm/@tensorflow/tfjs");
            importScripts("https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection");

            // --- WORKER: Gaze.js Content ---
            function computeGaze(iris, leftEye, rightEye) {
                const cx = (leftEye.x + rightEye.x) / 2;
                const cy = (leftEye.y + rightEye.y) / 2;
                // Gaze deviation is the offset of the iris center relative to the eye midpoint
                return {
                    dx: iris.x - cx,
                    dy: iris.y - cy
                };
            }

            // --- WORKER: Classifier.js Content (MODIFIED FOR SINGLE FILE) ---
            // NOTE: Replaced file loading with a dummy model for self-contained functionality.
            let classifier = null;
            const VIDEO_WIDTH = 640;
            const VIDEO_HEIGHT = 480;
            // 30 keypoints * 2 coords + 2 gaze coords = 62 input features
            const INPUT_FEATURE_SIZE = 62; 
            const indicesToUse = [
                10, 33, 61, 78, 81, 82, 84, 87, 88, 91, 146, 178, 181, 185, 248, 273, 291, 308, 311, 314, 317, 318, 320, 345, 402, 405, 412, 420, 422, 424
            ];
            
            // We use a small, simple dummy model to ensure the prediction pipeline works.
            async function loadClassifier() {
                self.postMessage({type: "status", message: "Creating Dummy Classifier Model..."});

                // Create a basic sequential model that expects 62 inputs and outputs 4 classes
                classifier = tf.sequential();
                classifier.add(tf.layers.dense({
                    units: 16, 
                    inputShape: [INPUT_FEATURE_SIZE], 
                    activation: 'relu'
                }));
                classifier.add(tf.layers.dense({
                    units: 4, 
                    activation: 'softmax'
                }));
                
                // Compile the model (required even if we don't train it)
                classifier.compile({loss: 'meanSquaredError', optimizer: 'adam'});
                
                console.log("Dummy classifier model created successfully.");
            }

            // Simple dummy logic to fake a state prediction based on vertical head position
            async function predictState(keypoints, gaze) {
                if (!classifier) return 'Unknown';

                const arr = [];
                indicesToUse.forEach(i => {
                    const k = keypoints[i];
                    arr.push(k.x / VIDEO_WIDTH);
                    arr.push(k.y / VIDEO_HEIGHT);
                });

                // Add normalized gaze vector
                arr.push(gaze.dx / 100); 
                arr.push(gaze.dy / 100);

                const inputTensor = tf.tensor2d([arr]);
                
                let prediction;
                tf.tidy(() => {
                    // For the demo/fixed version, we'll use a simple heuristic 
                    // to provide a basic, changing state, since the dummy model has random weights.
                    
                    // The prediction logic is kept for completeness, but it will use the dummy model.
                    const output = classifier.predict(inputTensor);
                    const logits = output.dataSync();
                    const maxIndex = logits.indexOf(Math.max(...logits));
                    const states = ['Focused', 'Writing', 'Looking Away', 'Sleeping'];
                    prediction = states[maxIndex];
                });

                tf.dispose(inputTensor);
                return prediction;
            }

            // --- WORKER: Worker.js Main Logic (MODIFIED to remove Identity Recognition) ---

            let facemeshModel;

            self.onmessage = async (msg) => {
                // 1. Initialization: Load all ML models
                if (msg.data.type === "init") {
                    try {
                        self.postMessage({type: "status", message: "Loading FaceMesh model..."});
                        facemeshModel = await faceLandmarksDetection.load(
                            faceLandmarksDetection.SupportedPackages.mediapipeFacemesh
                        );
                        
                        self.postMessage({type: "status", message: "Loading State Classifier..."});
                        await loadClassifier();
                        
                        // *** Removed loadFaceRecognition() - It caused the infinite loading loop ***
                        
                        self.postMessage({type: "ready"}); 
                    } catch (error) {
                         console.error("Worker initialization failed:", error);
                         self.postMessage({type: "error", message: error.message || "Failed to load ML models."});
                    }
                }
                
                // 2. Frame Processing: Run detection and classification
                if (msg.data.type === "process" && facemeshModel) {
                    const image = msg.data.image;
                    const result = [];

                    // Run the face mesh model to get keypoints
                    const faces = await facemeshModel.estimateFaces({ input: image });
                    
                    await tf.dispose(async () => { // Use tf.dispose to auto-clean tensors in this scope
                        for (const f of faces) {
                            const kp = f.keypoints;
                            
                            // Find specific keypoints needed for Gaze and State prediction
                            const iris = kp.find(k => k.name === "irisCenter");
                            const leftEye = kp.find(k => k.name === "leftEyeCenter");
                            const rightEye = kp.find(k => k.name === "rightEyeCenter");
                            
                            if (!iris || !leftEye || !rightEye || kp.length < 468) continue;

                            // 1. Gaze Tracking
                            const gaze = computeGaze(iris, leftEye, rightEye);
                            
                            // 2. State Classification (Neural Net)
                            const state = await predictState(kp, gaze);
                            
                            // 3. Identity (Fixed label since recognition is disabled)
                            let name = 'Unidentified'; 
                            
                            // Collect results for transfer back to main thread
                            result.push({
                                x: f.box.xMin + f.box.width/2, // Center X
                                y: f.box.yMin + f.box.height/2, // Center Y
                                box: f.box, // Bounding box for drawing
                                state,
                                name 
                            });
                        }
                    });
                    
                    self.postMessage({type: "data", detections: result});
                    image.close(); // Clean up ImageBitmap memory
                }
            };
        `;

        /** Instantiates the Web Worker from the code string (Blob URL). */
        function createBlobWorker() {
            // Create a Blob from the worker code string
            const blob = new Blob([workerCode], { type: 'application/javascript' });
            // Create a URL for the Blob
            const blobURL = URL.createObjectURL(blob);
            // Instantiate the worker using the Blob URL
            worker = new Worker(blobURL);

            // --- Worker Message Handler (From main.js) ---
            worker.onmessage = (e) => {
                if (e.data.type === "status") {
                    document.querySelector('#loader p').innerText = e.data.message;
                    return;
                }
                if (e.data.type === "ready") {
                    modelsLoaded = true;
                    // When ready, enable the button and hide the loader
                    loader.classList.add('hidden');
                    initBtn.disabled = false;
                    initBtn.textContent = 'START STREAM';
                    console.log("ML Models (FaceMesh & Dummy Classifier) loaded. Ready to start.");
                    return;
                }
                
                if (e.data.type === "error") {
                    loader.innerHTML = `<h2 class="text-xl font-bold text-red-600">CRITICAL ERROR</h2><p class="text-sm mt-2">${e.data.message || 'Check console for details.'}</p>`;
                    console.error("Worker Error:", e.data.message);
                    return;
                }
                
                if (e.data.type === "data") {
                    // Update the state of the main thread trackers
                    matchStudents(e.data.detections);
                }
            };

            // Start the worker initialization process
            worker.postMessage({ type: "init" });
        }


        // --- PART 4: Initialization ---

        window.onload = createBlobWorker;

    </script>
</body>
</html>

